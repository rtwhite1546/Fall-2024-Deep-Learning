{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTH 4320 / 5320 - Homework 1\n",
    "\n",
    "## Gradient Descent, Linear Models, and Logistic Classification\n",
    "\n",
    "**Deadline**: Sept 23\n",
    "\n",
    "**Points**: 70\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Submit **one** Python notebook file for grading. Your file must include **text explanations** of your work, **well-commented code**, and the **outputs** from your code.\n",
    "\n",
    "### Problems\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "1. [10 points] Write a version of gradient descent that chooses $n$ random starting points and outputs the parameters resulting in minimum training loss across all the runs.\n",
    "\n",
    "2. [5 points] Write a version of gradient descent that uses *momentum*.\n",
    "\n",
    "3. [10 points] Implement a linear regression class in the style of `scikit-learn` regressors (i.e. as a Python class with `fit` and `predict` functions), with capabilities to use $n$ random starting points and momentum (default to not using these features).\n",
    "\n",
    "4. [10 points] Train and tune a linear regression model to predict death rates from cancer in US counties using the [OLS Regression Challenge](https://data.world/nrippner/ols-regression-challenge). During tuning, perform hyperparameter searches with $n$ starting points, momentum, both, and neither. Use data from 30 random U.S. states for training, 10 random U.S. states for validation, and 10 random U.S. states for testing. Compare the results in terms of test accuracy, `fit` runtime, and `predict` runtime.\n",
    "\n",
    "#### Logistic Classification\n",
    "\n",
    "5. [10 points] Derive an explicit formula for the gradient of the sum of squared errors loss for a logistic classifier with $K$ classes for input data of dimension $d+1$ and one-hot labels. [*Hint.* Use the softmax function instead of sigmoid.]\n",
    "\n",
    "6. [5 points] Repeat Problem 5 with the cross-entropy loss.\n",
    "\n",
    "7. [10 points] Write a multi-class logistic classifier (for $K$ classes) in the style of `scikit-learn` classifiers (i.e. as a Python class with `fit` and `predict` functions), optimized by gradient descent with options to use multiple starting points and momentum.\n",
    "\n",
    "8. [10 points] Train and tune a logistic classifier model to predict the digits of the MNIST dataset. During tuning, perform hyperparameter searches with $n$ starting points, momentum, both, and neither. Use a 60\\% / 20\\% / 20\\% for training / validation / testing split. Compare the results in terms of test performance (confusion matrix and classification report), `fit` runtime, and `predict` runtime."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
