{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basic Introduction to Pytorch** \n",
    "## Prepared by: **Lamine Deen**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Introduction to PyTorch**\n",
    "\n",
    "What is PyTorch?\n",
    "\n",
    "- PyTorch is a flexible and powerful library for deep learning. It allows for the building of neural networks, handling tensors (multi-dimensional arrays), and performing automatic differentiation with ease.\n",
    "\n",
    "Why PyTorch for Deep Learning?\n",
    "\n",
    "- Easy to understand and debug with dynamic computation graphs.\n",
    "- Rich support for GPU acceleration.\n",
    "- Active community and robust ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Tensors: The Foundation of PyTorch**\n",
    "\n",
    "What is a Tensor?\n",
    "\n",
    "- In PyTorch, a tensor is the fundamental data structure used for building deep learning models. It can represent data in many dimensions—scalars, vectors, matrices, and higher-dimensional generalizations.\n",
    "\n",
    "- Scalar: A 0D tensor (e.g., 5).\n",
    "\n",
    "- Vector: A 1D tensor (e.g., [1, 2, 3]).\n",
    "\n",
    "- Matrix: A 2D tensor (e.g., [[1, 2], [3, 4]]).\n",
    "\n",
    "Let's start by creating some basic tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Scalar tensor (0D)\n",
    "scalar = torch.tensor(5)\n",
    "print(scalar)\n",
    "\n",
    "# Vector tensor (1D)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "print(vector)\n",
    "\n",
    "# Matrix tensor (2D)\n",
    "matrix = torch.tensor([[1, 2], [3, 4]])\n",
    "print(matrix)\n",
    "\n",
    "# 3D tensor (3D)\n",
    "tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "print(tensor3d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Tensor Operations\n",
    "\n",
    "Once we have tensors, we can perform operations like addition, multiplication, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n",
      "tensor([ 4., 10., 18.])\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor addition\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "z = x + y\n",
    "print(z)  # Output: tensor([5., 7., 9.])\n",
    "\n",
    "# Tensor multiplication (element-wise)\n",
    "z = x * y\n",
    "print(z)  # Output: tensor([4., 10., 18.])\n",
    "\n",
    "# Matrix multiplication (dot product)\n",
    "mat1 = torch.tensor([[1, 2], [3, 4]])\n",
    "mat2 = torch.tensor([[5, 6], [7, 8]])\n",
    "result = torch.mm(mat1, mat2)  # or mat1 @ mat2\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Moving Tensors to GPU\n",
    "\n",
    "One of the major advantages of PyTorch is its easy GPU support for tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available for linux machines\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"cuda\")\n",
    "# Check if a GPU is available for macOS machines\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"apple silicon\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"no silicon, no cuda\")\n",
    "\n",
    "\n",
    "# Creating a tensor on GPU\n",
    "x = torch.tensor([1, 2, 3], device=device)\n",
    "\n",
    "# Moving a tensor to GPU\n",
    "y = torch.tensor([4, 5, 6])\n",
    "y = y.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Autograd: Automatic Differentiation in PyTorch**\n",
    "\n",
    "In deep learning, we need to calculate gradients during backpropagation to update model parameters. PyTorch simplifies this process with its Autograd module, which automatically computes gradients of tensors involved in computational graphs.\n",
    "\n",
    "What is Autograd?\n",
    "\n",
    "- Autograd is PyTorch’s automatic differentiation engine. It tracks operations performed on tensors and automatically computes their derivatives during backpropagation.\n",
    "- When you perform operations on tensors, PyTorch builds a computational graph in the background. This graph stores information about operations and allows for efficient gradient computation.\n",
    "\n",
    "Key Concept: Computational Graph\n",
    "\n",
    "- The computational graph represents all operations and tensors as nodes and edges. Each operation applied to a tensor creates a new tensor, and PyTorch tracks these operations.\n",
    "- When you call .backward() on a loss, PyTorch traverses this graph in reverse to compute gradients for every parameter involved in the graph.\n",
    "\n",
    "Tracking Gradients with requires_grad\n",
    "\n",
    "- By default, PyTorch does not track operations for gradients. To enable gradient tracking, you need to set requires_grad=True for a tensor.\n",
    "- Tensors with requires_grad=True will accumulate gradients as you perform operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11., 19.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([7., 9.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with requires_grad=True\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True, device=device)\n",
    "\n",
    "# Perform operations on the tensor\n",
    "y = x ** 2 + 3 * x + 1\n",
    "print(y)  # Outputs the result of the operations\n",
    "\n",
    "# Compute gradients by calling .backward() on the output\n",
    "y.sum().backward()  # You need to reduce y to a scalar before calling backward\n",
    "print(x.grad)  # Outputs the gradient (dy/dx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using torch.no_grad() for Inference: When you don’t need to compute gradients (for example, during inference), you can disable autograd with torch.no_grad(). This saves memory and speeds up computation by preventing the creation of the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Perform operations without tracking gradients\n",
    "    y = x ** 2 + 3 * x + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Neural Networks with torch.nn**\n",
    "\n",
    "In PyTorch, neural networks are built using the torch.nn module, which provides:\n",
    "\n",
    "- Layers: Predefined building blocks like fully connected layers, convolutional layers, etc.\n",
    "\n",
    "- Activation Functions: Functions that introduce non-linearity, allowing neural networks to solve complex problems.\n",
    "\n",
    "- Loss Functions and Optimizers: Tools to define objectives and methods for updating model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define a Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (fc2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the network by subclassing nn.Module\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Define layers: a fully connected layer with 2 inputs and 3 outputs\n",
    "        self.fc1 = nn.Linear(2, 3)  # input size = 2, output size = 3\n",
    "        self.fc2 = nn.Linear(3, 1)  # input size = 3, output size = 1\n",
    "\n",
    "    # Define forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # Pass through the first layer\n",
    "        x = torch.relu(x)  # Apply activation function (ReLU)\n",
    "        x = self.fc2(x)  # Pass through the second layer\n",
    "        return x\n",
    "\n",
    "# Initialize the network\n",
    "net = SimpleNN()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "nn.Linear(in_features, out_features): This defines a fully connected layer, where each input is connected to each output. In this example:\n",
    "\n",
    "- fc1 connects a 2-dimensional input to a 3-dimensional output.\n",
    "\n",
    "- fc2 connects the 3-dimensional output from fc1 to a 1-dimensional output.\n",
    "\n",
    "forward() method: This method defines how data flows through the network. In the example:\n",
    "\n",
    "- Data is passed through the first fully connected layer (fc1).\n",
    "\n",
    "- A ReLU activation function is applied to introduce non-linearity.\n",
    "\n",
    "- The output is passed through the second fully connected layer (fc2).\n",
    "\n",
    "Activation Functions: These functions are used to introduce non-linearity. Without them, a neural network would just be a linear model. Common activation functions include:\n",
    "\n",
    "- ReLU (Rectified Linear Unit): Sets all negative values to zero and leaves positive values unchanged. It is widely used in hidden layers.\n",
    "\n",
    "- Sigmoid and Tanh: Typically used in the output layers for binary classification or multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create Input Data and Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2837]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a sample input tensor (batch size of 1, 2 features)\n",
    "input_data = torch.tensor([[1.0, 2.0]])\n",
    "\n",
    "# Perform a forward pass (getting the output)\n",
    "output = net(input_data)\n",
    "print(output)  # Output will be a tensor of shape (1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- Input Shape: We pass in a tensor with shape (1, 2), meaning a batch size of 1 and 2 input features (matching the fc1 layer’s input size).\n",
    "\n",
    "- Forward Pass: When calling net(input_data), PyTorch automatically executes the forward() method, passing the data through the layers and returning the network's output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a neural network, we need:\n",
    "\n",
    "A loss function to quantify how far the predictions are from the actual values.\n",
    "An optimizer to update the model's weights to minimize the loss.\n",
    "\n",
    "Let’s use Mean Squared Error (MSE) loss and Stochastic Gradient Descent (SGD) for optimization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Define a Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function (Mean Squared Error)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)  # Learning rate = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- nn.MSELoss(): This is used when the task is regression. It calculates the mean squared error between the predicted and target values.\n",
    "\n",
    "- torch.optim.SGD(): The optimizer adjusts the weights of the network to minimize the loss. It takes the model parameters (net.parameters()) and a learning rate (lr=0.01).\n",
    "\n",
    "- The net.parameters() are the weights and biases associates with each layer from  input to output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Training the Network\n",
    "\n",
    "The training process involves:\n",
    "\n",
    "1. Forward Pass: Compute predictions.\n",
    "2. Compute Loss: Compare predictions with actual values using the loss function.\n",
    "3. Backward Pass: Compute gradients via backpropagation.\n",
    "4. Update Weights: Use the optimizer to adjust weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5130363702774048\n",
      "Epoch 10, Loss: 0.20748411118984222\n",
      "Epoch 20, Loss: 0.08128686249256134\n",
      "Epoch 30, Loss: 0.0293605774641037\n",
      "Epoch 40, Loss: 0.009833571501076221\n",
      "Epoch 50, Loss: 0.0031189327128231525\n",
      "Epoch 60, Loss: 0.0009556243312545121\n",
      "Epoch 70, Loss: 0.00028686862788163126\n",
      "Epoch 80, Loss: 8.512281783623621e-05\n",
      "Epoch 90, Loss: 2.509541081963107e-05\n"
     ]
    }
   ],
   "source": [
    "# Sample target output\n",
    "target = torch.tensor([[1.0]])\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(100):  # Train for 100 epochs\n",
    "    optimizer.zero_grad()  # Zero the gradient buffers\n",
    "\n",
    "    output = net(input_data)  # Forward pass: compute the output\n",
    "    loss = criterion(output, target)  # Compute the loss\n",
    "\n",
    "    loss.backward()  # Backward pass: compute the gradients\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 10 == 0:  # Print loss every 10 epochs\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- optimizer.zero_grad(): Resets the gradients to zero. This is necessary because gradients accumulate by default in PyTorch.\n",
    "\n",
    "- loss.backward(): Computes the gradients of the loss with respect to the model parameters.\n",
    "\n",
    "- optimizer.step(): Updates the model parameters using the gradients.\n",
    "\n",
    "- The training loop runs for 100 epochs, printing the loss every 10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients accumulate by default in PyTorch.\n",
    "- optimizer.zero_grad() is called to reset gradients after each batch, ensuring that previous batches' gradients don’t interfere with the current batch's gradients.\n",
    "- Accumulating gradients is useful for specific cases (like gradient accumulation or mini-batch strategies), but in most cases, resetting gradients each batch is necessary for proper parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Evaluate the Model\n",
    "- Once the model has been trained, you can use it to make predictions on new data. When making predictions, we don’t need to compute gradients, so we wrap the code in torch.no_grad() to save memory and improve efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tensor([[1.6793]])\n"
     ]
    }
   ],
   "source": [
    "# New input data\n",
    "test_data = torch.tensor([[4.0, 5.0]])\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    prediction = net(test_data)\n",
    "    print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Working with Data in PyTorch**\n",
    "\n",
    "Handling data efficiently is critical. PyTorch provides two key abstractions to help with this: Dataset and DataLoader. These allow for efficient loading, preprocessing, and batching of data, which is essential for training models on large datasets.\n",
    "\n",
    "Step 1: The Dataset Class\n",
    "\n",
    "The Dataset class in PyTorch provides an abstraction for datasets. You can either use built-in datasets (like MNIST, CIFAR-10) or create a custom dataset by subclassing torch.utils.data.Dataset.\n",
    "\n",
    "Built-in Datasets Example\n",
    "Let's start by loading a simple dataset like MNIST, a popular dataset of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transformation to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explanation:\n",
    "\n",
    "transforms.Compose() allows you to combine multiple transformations. In this case, we are using two: ToTensor() and Normalize().\n",
    "\n",
    "ToTensor()\n",
    "\n",
    "- It changes the image's shape from (Height, Width, Channels) (typical for images) to (Channels, Height, Width) (expected format for PyTorch).\n",
    "- It also scales pixel values from the range [0, 255] (common in image formats) to the range [0, 1], which is essential for efficient training.\n",
    "\n",
    "Normalize()\n",
    "\n",
    "- Normalizing the inputs ensures that the neural network’s input distribution has zero mean and a standard deviation close to 1, which improves the convergence of the training process.\n",
    "\n",
    "datasets.MNIST: \n",
    "Downloads the MNIST dataset and applies the transformations. The train=True argument indicates we're loading the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Dataset Example\n",
    "\n",
    "You can also create a custom dataset by subclassing torch.utils.data.Dataset.  \n",
    "This is useful when your data is stored in formats like CSV files, image folders, or other custom formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data  # Your input data\n",
    "        self.labels = labels  # Corresponding labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Return the size of the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a tuple of (input, label) for the given index\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Example data\n",
    "data = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0], [7.0, 8.0], [9.0, 3.0]])\n",
    "labels = torch.tensor([0, 1, 1, 0, 0])\n",
    "\n",
    "# Create an instance of MyDataset\n",
    "dataset = MyDataset(data, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:  \n",
    "- __len__(): Returns the total number of samples in the dataset.\n",
    "- __getitem__(): Returns a single sample (input and label) from the dataset based on the index provided (idx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: The DataLoader Class\n",
    "\n",
    "The DataLoader class is used to load data in batches and shuffle it, which is crucial for training models efficiently. It handles:\n",
    "\n",
    "Batching: Grouping multiple samples into batches.  \n",
    "Shuffling: Randomizing the order of data at each epoch to improve learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "Inputs: tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "Labels: tensor([0, 1])\n",
      "Batch 1:\n",
      "Inputs: tensor([[5., 6.],\n",
      "        [7., 8.]])\n",
      "Labels: tensor([1, 0])\n",
      "Batch 2:\n",
      "Inputs: tensor([[9., 3.]])\n",
      "Labels: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader for the custom dataset\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Labels: {labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- batch_size: The number of samples in each batch. In this case, 2 samples are loaded in each batch.  \n",
    "- shuffle=True: Randomizes the order of the data at the start of each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Putting It All Together\n",
    "\n",
    "Let’s integrate everything we’ve learned so far (building a simple neural network and handling data) into a complete example. We’ll use the MNIST dataset, create a neural network, and train it using a DataLoader and test it.\n",
    "\n",
    "Code Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.4330\n",
      "Epoch 2, Training Loss: 0.4072\n",
      "Epoch 3, Training Loss: 0.1734\n",
      "Epoch 4, Training Loss: 0.1398\n",
      "Epoch 5, Training Loss: 0.3940\n",
      "\n",
      "Test Loss: 0.2557, \n",
      "Test Accuracy: 92.68%\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # Fully connected layer (input: 28x28 pixels, output: 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output layer (10 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU activation after first layer\n",
    "        x = self.fc2(x)  # Linear output\n",
    "        return x\n",
    "\n",
    "# Define transformations (same for train and test)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the MNIST training and test datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the network, loss function, and optimizer\n",
    "net = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Train for 5 epochs\n",
    "    net.train()  # Set the network to training mode\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.view(-1, 28 * 28)  # Flatten the images\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        outputs = net(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the weights\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing loop\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0.0\n",
    "\n",
    "net.eval()  # Set the network to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.view(-1, 28 * 28)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "avg_loss = test_loss / total\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'\\nTest Loss: {avg_loss:.4f}, \\nTest Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- Dataset: We load the MNIST dataset, applying transformations to convert images to tensors and normalize them.\n",
    "- Train DataLoader: We use the DataLoader to shuffle and load the data in batches of 64 samples at a time.\n",
    "- Test Dataloader:  \n",
    "                 - train=False: This ensures that we are loading the test split of the MNIST dataset, not the training data.  \n",
    "                 - shuffle=False: We don’t need to shuffle the test data because we are not training on it; we’re simply evaluating the model's performance on each sample.\n",
    "- Neural Network: The network has two layers. The input layer takes a 28x28 pixel image (flattened into a vector of size 784), and the output layer predicts one of 10 classes (digits 0–9).\n",
    "- Training Loop: For each epoch, we iterate over the data, compute the loss using cross-entropy, and update the model’s parameters using stochastic gradient descent.\n",
    "- optimizer.zero_grad() is called before each batch to reset the gradients, ensuring that the gradients for the current batch are calculated independently of previous batches.  \n",
    "If we don’t reset the gradients, they accumulate across batches, which leads to incorrect weight updates and unstable training.  \n",
    "Gradient accumulation can be useful when simulating larger batch sizes by accumulating gradients over multiple small batches, but in most cases, zeroing the gradients before each batch is the correct approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluating the Model  \n",
    "net.eval(): This sets the model to evaluation mode. In this mode, some layers like dropout or batch normalization behave differently from training mode. It’s important to switch to evaluation mode during testing to ensure these layers behave correctly.  \n",
    "To evaluate the model on the test data, we follow these steps:  \n",
    "          - torch.no_grad(): During evaluation, we don’t need to compute gradients (since we’re not updating the model). This context manager disables gradient tracking, making the process more memory-efficient and faster.  \n",
    "          - Calculate accuracy or other performance metrics by comparing the model's predictions to the actual labels.  \n",
    "          - Average the loss over the entire test dataset.  \n",
    "          - Loss Calculation:  \n",
    "We compute the loss for each batch and multiply it by the batch size (inputs.size(0)) to accumulate the total loss for the entire dataset.  \n",
    "At the end, we divide the total loss by the number of samples to get the average loss over the test dataset.  \n",
    "          - Accuracy Calculation:  \n",
    "torch.max(outputs, 1) returns the predicted class with the highest score for each sample in the batch.  \n",
    "We compare the predicted class to the actual class labels (predicted == labels) and sum the number of correct predictions.  \n",
    "Accuracy is computed as the ratio of correct predictions to total predictions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 (Step 3) Optimized for acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "- Check GPU Availability: We will check whether a GPU is available on the system (using MPS for Apple Silicon or CUDA for NVIDIA GPUs).\n",
    "- Move Data and Model to GPU: Both the neural network and the input data need to be moved to the GPU to leverage its acceleration.\n",
    "- Move Output and Gradients Back to CPU: After computation, if you need to print or log values, it’s often necessary to move data back to the CPU.  \n",
    "\n",
    "Steps to Run on the GPU:\n",
    "- Move the model to GPU: Use .to(device) where device is either MPS, CUDA, or CPU.\n",
    "- Move input and target tensors to the GPU during training and testing.\n",
    "- Make sure the optimizer updates the model on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.4639\n",
      "Epoch 2, Training Loss: 0.2711\n",
      "Epoch 3, Training Loss: 0.1035\n",
      "Epoch 4, Training Loss: 0.1338\n",
      "Epoch 5, Training Loss: 0.1750\n",
      "\n",
      "Test Loss: 0.2637, \n",
      "Test Accuracy: 92.43%\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use MPS for Apple Silicon\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use CUDA for NVIDIA GPUs\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Use CPU if no GPU is available\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # Fully connected layer (input: 28x28 pixels, output: 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output layer (10 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU activation after first layer\n",
    "        x = self.fc2(x)  # Linear output\n",
    "        return x\n",
    "\n",
    "# Define transformations (same for train and test)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the MNIST training and test datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the network, loss function, and optimizer\n",
    "net = SimpleNN().to(device)  # Move the network to the GPU\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Train for 5 epochs\n",
    "    net.train()  # Set the network to training mode\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.view(-1, 28 * 28).to(device), labels.to(device)  # Move inputs and labels to the GPU\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        outputs = net(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the weights\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing loop\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0.0\n",
    "\n",
    "net.eval()  # Set the network to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.view(-1, 28 * 28).to(device), labels.to(device)  # Move inputs and labels to the GPU\n",
    "        outputs = net(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n",
    "        correct += (predicted == labels).sum().item()  # Track the number of correct predictions\n",
    "        total += labels.size(0)\n",
    "\n",
    "avg_loss = test_loss / total\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'\\nTest Loss: {avg_loss:.4f}, \\nTest Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of GPU Usage:\n",
    "- Detect GPU Availability:\n",
    "\n",
    "We check if an MPS device (for Apple Silicon) or CUDA device (for NVIDIA GPUs) is available. If neither is available, the code defaults to using the CPU.\n",
    "- Move Model to GPU:\n",
    "\n",
    "The model is moved to the GPU with net = SimpleNN().to(device). This ensures that all operations (forward pass, backward pass, etc.) are performed on the GPU.\n",
    "- Move Data to GPU:\n",
    "\n",
    "Before each forward pass, the input data (inputs) and the target labels (labels) are moved to the GPU using .to(device).\n",
    "Important: You must move both the model and the data to the same device (either CPU or GPU) for operations to work.\n",
    "- Testing on the GPU:\n",
    "\n",
    "During testing, we also move the input data and labels to the GPU using .to(device) before making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits of Using the GPU:\n",
    "- Faster Computation: GPUs are optimized for parallel computation, making operations like matrix multiplication (which are common in deep learning) much faster.\n",
    "- Efficient Training: Training deep learning models on large datasets, especially with complex architectures, is significantly faster when using GPUs.\n",
    "- Noticeable Difference: you will notice a difference in computation time when running models with a medium sizes and not for small models.  \n",
    "\n",
    "The GPU will be slower for this particular task due to the small model size, small batch size, and data transfer overhead. For small-scale tasks like MNIST, the CPU can sometimes perform the same or even faster than the GPU. For larger models and datasets, the GPU will outperform the CPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
